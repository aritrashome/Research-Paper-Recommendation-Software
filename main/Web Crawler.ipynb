{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL PDF Parser\n",
    "import urllib\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "\n",
    "def pdf_from_url_to_txt(url):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr,  laparams=laparams)\n",
    "    f = urllib.request.urlopen(url).read()\n",
    "    fp = BytesIO(f)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "    for page in PDFPage.get_pages(fp,\n",
    "                                  pagenos,\n",
    "                                  maxpages=maxpages,\n",
    "                                  password=password,\n",
    "                                  caching=caching,\n",
    "                                  check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    str = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query :computer\n",
      "https://www.sonycsl.co.jp/person/rekimoto/papers/uist95.pdf\n",
      "The world through the computer: Computer augmented interaction with real world environments\n",
      "--------\n",
      "----------------------------\n",
      "https://apps.dtic.mil/dtic/tr/fulltext/u2/707853.pdf\n",
      "THE ALOHA SYSTEM: another alternative for computer communications\n",
      "--------\n",
      "----------------------------\n",
      "https://patentimages.storage.googleapis.com/f1/6d/05/25f3f2c69c74b8/US5049862.pdf\n",
      "Keyless flat panel portable computer--computer aided notebook\n",
      "--------\n",
      "----------------------------\n",
      "http://www.ivanpoupyrev.com/e-library/2004/CHI2004_gummi.pdf\n",
      "Gummi: a bendable computer\n",
      "--------\n",
      "----------------------------\n",
      "http://paulbourke.net/fractals/peterdejong/peterdejong.pdf\n",
      "Computer recreations\n",
      "--------\n",
      "----------------------------\n",
      "https://www.researchgate.net/profile/Alistair_Cockburn/publication/2955526_Agile_software_development_The_people_factor/links/56d434b908ae868628b2453c/Agile-software-development-The-people-factor.pdf\n",
      "Agile software development, the people factor\n",
      "https://nissenbaum.tech.cornell.edu/papers/How%20Computer%20Systems%20Embody%20Values.pdf\n",
      "How computer systems embody values\n",
      "--------\n",
      "----------------------------\n",
      "http://www.academia.edu/download/4677797/issues_in_emerging_4g_wireless_networks.pdf\n",
      "Issues in emerging 4G wireless networks\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "query = input(\"Enter query :\")\n",
    "url = \"https://scholar.google.com/scholar?start=0&q=\"+str(query)+\"computer&hl=en&as_sdt=0,5\"\n",
    "\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "data = []\n",
    "for item in soup.select('[data-did]'):\n",
    "    paper = []  #1.Pdf link  2.title  3.Keywords(10) 4.body\n",
    "    try:\n",
    "        #print(str(item.select('div.gs_ggsd')[0].select('a')[0]['href']))  #Pdf link\n",
    "        if(re.search('.pdf',item.select('div.gs_ggsd')[0].select('a')[0]['href'])):\n",
    "            print(str(item.select('div.gs_ggsd')[0].select('a')[0]['href']))  #Pdf link\n",
    "            paper.append(str(item.select('div.gs_ggsd')[0].select('a')[0]['href']))\n",
    "        else: continue\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    print(item.select('h3')[0].get_text()) #Title\n",
    "    paper.append(item.select('h3')[0].get_text())\n",
    "    try:\n",
    "        text = pdf_from_url_to_txt(paper[0])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # importing modules \n",
    "    from nltk.stem import PorterStemmer \n",
    "    from nltk.tokenize import word_tokenize \n",
    "    regex = re.compile('[^a-zA-Z.,0-9]')\n",
    "    text = regex.sub(' ',text)\n",
    "    #print(text)\n",
    "\n",
    "    from rake_nltk import Rake\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    r = Rake(punctuations=\".,\"'',stopwords=stopwords.words('english'),max_length=1) # Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "\n",
    "    r.extract_keywords_from_text(text)\n",
    "\n",
    "    tmp = r.get_ranked_phrases()[:5] # To get keyword phrases ranked highest to lowest.\n",
    "    keyword = \"; \".join(tmp)\n",
    "    \n",
    "    paper.append(keyword)\n",
    "    try:\n",
    "        paper.append(text[:1000])\n",
    "    except:\n",
    "        paper.append(text)\n",
    "    data.append(paper)\n",
    "    #print(item.select('a')[0]['href'])\n",
    "    print('--------')\n",
    "    #print(item.select('.gs_rs')[0].get_text())   #Abstract\n",
    "    #print(item.select('.gs_a')[0].get_text())   #Green text\n",
    "    \n",
    "    print('----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data,columns=['Link','Source title','Index Keywords','Abstract'])\n",
    "df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "########################### KDM ##########################\n",
    "import subprocess\n",
    "import csv\n",
    "subprocess.run('python kdm.py',shell=True)\n",
    "fileName=\"paper_ranks.csv\"\n",
    "\n",
    "     \n",
    "with open(fileName,encoding='latin-1') as File:\n",
    "    reader = csv.DictReader(File)\n",
    "    results = [ row for row in reader ]\n",
    "#print(results[0],\"\\n\",results[1])\n",
    "\n",
    "df['KDM'] = 0\n",
    "for row in results:\n",
    "    idx = row.get('No',None)\n",
    "    conf = row.get('Confidence',None)\n",
    "    #print(idx,' ',conf,'\\n')\n",
    "    try:\n",
    "        df['KDM'][int(idx)] = float(conf)\n",
    "    except: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "################################################ CAOT #################################\n",
    "# coding: utf-8\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "#All the header files required by the file\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "import pandas as pd\n",
    "from scipy import stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "#from selenium.webdriver import browser\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from difflib import SequenceMatcher\n",
    "import datetime\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "#get_ipython().magic('matplotlib inline')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "#The name of the csv file to be opened\n",
    "#readDataFileLoc = \"scopus_2000_Citation.csv\"\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "#Reading the csv file and doing the required preprocessing\n",
    "#It takes only the required columns - Authors, Title, Year, Cited by\n",
    "#Removing the rows i.e. papers whose citations are 0\n",
    "citationDataset = pd.read_csv(\"data.csv\",)\n",
    "#citationDataset=citationDataset1.iloc[:10,:]\n",
    "citationDataset.head()\n",
    "#newCitationDataset = citationDataset[['Authors', 'Title', 'Year', 'Cited by']]\n",
    "#print(newCitationDataset.head())\n",
    "#finalCitationDataset = newCitationDataset[pd.notnull(newCitationDataset['Cited by'])]\n",
    "#listCitationData = finalCitationDataset.values\n",
    "#print(len(listCitationData))\n",
    "#print(finalCitationDataset)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "def getScholarSearchResult(browser, queryToSearch):\n",
    "    \"\"\"\n",
    "    To type the query in the google scholar search bar and search it\n",
    "\n",
    "    Args:\n",
    "        browser: An open browser using selenium(preferably firefox)\n",
    "        queryToSearch: A string which has to be searched in the google scholar\n",
    "\n",
    "    Returns:\n",
    "        browser(no need actually)\n",
    "    \"\"\"\n",
    "    scholarSearchBar = browser.find_element_by_name(\"q\")\n",
    "    scholarSearchBar.send_keys(queryToSearch)\n",
    "    scholarSearchBar.submit()\n",
    "    return browser\n",
    "\n",
    "def searchScholarResults(browser, queryToSearch):\n",
    "    \"\"\"\n",
    "    To type the query in the google scholar search bar and search it\n",
    "\n",
    "    Args:\n",
    "        browser: An open browser using selenium(preferably firefox)\n",
    "        queryToSearch: A string which has to be searched in the google scholar\n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    scholarSearchBar = browser.find_element_by_name(\"q\")\n",
    "    scholarSearchBar.send_keys(queryToSearch)\n",
    "    scholarSearchBar.submit()\n",
    "    return\n",
    "\n",
    "def customSimilarityChecker(listpapers, paperTitle):\n",
    "    \"\"\"\n",
    "    A basic similarity checker which compares all the Names of paper displayed by the Scholar with the desired paper and clicks the link of the paper\n",
    "\n",
    "    Args:\n",
    "        listpapers: HTML listofpapers which contain the Title of the papers\n",
    "        paperTitle: Title of the paper which we are searching\n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    maxDiff = SequenceMatcher(None, listpapers[0].text, paperTitle).ratio()\n",
    "    #print(maxDiff)\n",
    "    for x in range(0, len(listpapers)):\n",
    "        temp = SequenceMatcher(None, listpapers[x].text, paperTitle).ratio()\n",
    "        if temp > maxDiff:\n",
    "            index = x\n",
    "            maxDiff = temp\n",
    "    listpapers[index].click()\n",
    "    return\n",
    "\n",
    "def getYearCitationInformation(browser):\n",
    "    \"\"\"\n",
    "    Calling this function will return an 1-d array containing pairs of year and citations in that year of the paper\n",
    "    Takes the open page of the paper in Google Scholar as input\n",
    "\n",
    "    Args:\n",
    "        browser: An open browser using selenium(preferably firefox)\n",
    "\n",
    "    Returns:\n",
    "        A 1-d array containing pairs of year and citations in that year of the paper\n",
    "    \"\"\"\n",
    "    answer = []\n",
    "    try:\n",
    "        idBars = browser.find_element_by_id('gsc_vcd_graph_bars')\n",
    "    except Exception:\n",
    "        return answer\n",
    "    barsList = idBars.find_elements_by_tag_name('a')\n",
    "    lengthBars = len(barsList)\n",
    "    listOfBarsYear = idBars.find_elements_by_class_name('gsc_vcd_g_t')\n",
    "    for i in range(0, lengthBars):\n",
    "        answer.append([(barsList[lengthBars - i - 1].find_elements_by_tag_name('span'))[0].get_attribute('textContent'),\n",
    "                       listOfBarsYear[lengthBars - i - 1].text])\n",
    "    \n",
    "    return answer\n",
    "\n",
    "#This function will click the more button repeteadly to display all the research papers published by a person on the page\n",
    "def correctClickMoreButton(browser):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        browser: \n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return\n",
    "\n",
    "def searchChooseCitationInfo(browser, queryToSearch, paperTitle):\n",
    "    \"\"\"\n",
    "    Calling this function will return an 1-d array containing pairs of year and citations in that year of the paper,\n",
    "     taking only an open browser as input\n",
    "    :param browser: An open browser using selenium(preferably firefox)\n",
    "    :param queryToSearch: The text query which we are going to search in the google scholar\n",
    "    :param paperTitle: Title of the paper which we are searching\n",
    "    :return: A 1-d array containing pairs of year and citations in that year of the paper\n",
    "    \"\"\"\n",
    "    browser.get(\"https://scholar.google.com/\")\n",
    "    scholarSearchBar = browser.find_element_by_name(\"q\")\n",
    "    scholarSearchBar.send_keys(queryToSearch)\n",
    "    scholarSearchBar.submit()\n",
    "    #tempList = browser.find_elements_by_class_name(\"gs_a\")\n",
    "    #tempList2 = tempList[0].find_elements_by_tag_name('a')\n",
    "    #tempList2[0].click()\n",
    "    time.sleep(5)\n",
    "    ((browser.find_elements_by_class_name(\"gs_a\"))[0].find_elements_by_tag_name('a'))[0].click()\n",
    "    time.sleep(5)\n",
    "    correctClickMoreButton(browser)\n",
    "    listpapers = browser.find_elements_by_class_name('gsc_a_at')\n",
    "    customSimilarityChecker(listpapers, paperTitle)\n",
    "    time.sleep(5)\n",
    "    return getYearCitationInformation(browser)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def writeListCitationToCsv(fileName, answer, listCitationData, score):\n",
    "    \"\"\"\n",
    "    Writes the citation data to the csv file\n",
    "\n",
    "    Args:\n",
    "        fileName: name of the file to which data has to be written\n",
    "        answer: citation data for each paper with respect to time\n",
    "        listCitationData: consist of related data of each paper\n",
    "        score: score for each paper\n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    max = 0\n",
    "    now = datetime.datetime.now()\n",
    "    start = int(now.year)\n",
    "    i = 0\n",
    "    for i in range(0, len(answer)):\n",
    "        if len(answer[i]) > 0:\n",
    "            t = start - int(answer[i][0][1]) + len(answer[i])\n",
    "        else:\n",
    "            t = len(answer[i])\n",
    "        if t > max:\n",
    "            max = t\n",
    "    f = open(fileName, 'w+')\n",
    "    csvout = csv.writer(f)\n",
    "    row = []\n",
    "    row.extend(['Authors', 'Title', 'Year',\n",
    "                'Cited by', 'Cited by updated', 'Last Citation Year', 'Score'])\n",
    "    rowInitData = len(row)\n",
    "    for i in range(0, max):\n",
    "        row.extend([start - i])\n",
    "    end = start - max + 1\n",
    "    csvout.writerow(row)\n",
    "    for i in range(0, len(answer)):\n",
    "        row = []\n",
    "        sum = 0\n",
    "        for m in range(0, len(answer[i])):\n",
    "            sum = sum + int(answer[i][m][0])\n",
    "        if len(answer[i]) > 0:\n",
    "            row.extend([listCitationData[i][0], listCitationData[i][1], listCitationData[i][2]\n",
    "                           , listCitationData[i][3], sum, answer[i][len(answer[i]) - 1][1], score[i]])\n",
    "        else:\n",
    "            row.extend([listCitationData[i][0], listCitationData[i][1], listCitationData[i][2]\n",
    "                           , listCitationData[i][3], 0, -1, score[i]])\n",
    "        j = 0\n",
    "        k = start\n",
    "        while ((j < len(answer[i])) and (k >= end)):\n",
    "            if (int(answer[i][j][1]) == k):\n",
    "                row.extend([answer[i][j][0]])\n",
    "                j = j + 1\n",
    "            else:\n",
    "                row.extend([0])\n",
    "            k = k - 1\n",
    "        incompIter = max - len(row) + rowInitData\n",
    "        for l in range(0, incompIter):\n",
    "            row.extend([0])\n",
    "        csvout.writerow(row)\n",
    "    return\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "#Opening the browser\n",
    "#self.driver = webdriver.Firefox()\n",
    "#opts = webdriver.chrome.options.Options()\n",
    "#opts.set_headless()\n",
    "#assert opts.headless\n",
    "browser = webdriver.Chrome(\"chromedriver.exe\")\n",
    "#browser = webdriver.Chrome(options=opts)\n",
    "browser.get(\"https://scholar.google.com/\")\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "#Collect the data\n",
    "answer = []\n",
    "for i in range(0, len(df)):\n",
    "    paperTitle = df[\"Source title\"][i]\n",
    "    \n",
    "    #paperAuthor = listCitationData[i][0]\n",
    "    try:\n",
    "        answer.append(searchChooseCitationInfo(browser, paperTitle , paperTitle))\n",
    "    except Exception:\n",
    "        answer.append([])\n",
    "    #print(i)\n",
    "browser.quit()\n",
    "#print(len(answer))\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "score = []\n",
    "def slopeMethodScoreRegression(answer, decay):\n",
    "    \"\"\"\n",
    "    Returns a score telling how relevant the paper is from its citation data\n",
    "\n",
    "    Args:\n",
    "        answer: Citation data of all the papers\n",
    "        decay: rate of decay of importance of citation data with respect to time\n",
    "\n",
    "    Returns:\n",
    "        score for every paper\n",
    "    \"\"\"\n",
    "    now = datetime.datetime.now()\n",
    "    currentYear = int(now.year)\n",
    "    \n",
    "    for i in range(0, len(answer)):\n",
    "        sum = 0\n",
    "        if len(answer[i]) > 0:\n",
    "            sum = 0\n",
    "            #sum = sum + int(answer[i][len(answer[i]) - 1][0])\n",
    "            #sum = sum + int(answer[i][0][0]) * int(answer[i][0][0])\n",
    "        else:\n",
    "            score.extend([-1000])\n",
    "            continue\n",
    "        lenreq = len(answer[i])\n",
    "        for j in range(0, lenreq - 1):\n",
    "            diff = int(answer[i][lenreq - j - 2][0]) - int(answer[i][lenreq - j - 1][0])\n",
    "            sum = diff * abs(diff) + sum * decay\n",
    "        sum = sum + int(answer[i][len(answer[i]) - 1][0]) * int(answer[i][len(answer[i]) - 1][0])\n",
    "        sum = sum + int(answer[i][0][0]) * int(answer[i][0][0])\n",
    "        avgScore = sum / (currentYear - int(answer[i][len(answer[i]) - 1][1]) + 1)\n",
    "        score.extend([avgScore])\n",
    "    return score\n",
    "\n",
    "score = slopeMethodScoreRegression(answer, 0.4)\n",
    "df[\"CAOT\"] = score\n",
    "#citationDataset.to_csv(\"data.csv\")\n",
    "#print(score)\n",
    "#score.append(score)\n",
    "#x=listCitationData.assign(CA=pd.Series(score))\n",
    "#print(x.head())\n",
    "#x.to_csv('scopus_mod_new4.csv')\n",
    "#writeListCitationToCsv('ansCit.csv', answer, listCitationData, score)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### SQM ###########################\n",
    "def SQM(answer):\n",
    "    score = []\n",
    "    for i in range(len(df)):\n",
    "        sqm = 0\n",
    "        if len(answer[i]):\n",
    "            for j in range(len(answer[i])-1):\n",
    "                slope = int(answer[i][j][0]) - int(answer[i][j+1][0])\n",
    "                amp = abs(slope)\n",
    "                sqm += slope*amp\n",
    "            \n",
    "        else:\n",
    "            sqm = -1000\n",
    "        score.append(sqm)\n",
    "    return score\n",
    "\n",
    "df['SQM'] = 0\n",
    "df['SQM'] = SQM(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Link</th>\n",
       "      <th>Source title</th>\n",
       "      <th>Index Keywords</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>CAOT</th>\n",
       "      <th>SCA</th>\n",
       "      <th>SQM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.sonycsl.co.jp/person/rekimoto/pape...</td>\n",
       "      <td>The world through the computer: Computer augme...</td>\n",
       "      <td>x; world; workstation; work; wish</td>\n",
       "      <td>The World through the Computer   Computer Augm...</td>\n",
       "      <td>19.476586</td>\n",
       "      <td>57</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://apps.dtic.mil/dtic/tr/fulltext/u2/7078...</td>\n",
       "      <td>THE ALOHA SYSTEM: another alternative for comp...</td>\n",
       "      <td>yj; xpl; written; would; world</td>\n",
       "      <td>AFOqSR  7 0  16 8 6TA  THE  ALOHA  SYSTEM    A...</td>\n",
       "      <td>-1000.000000</td>\n",
       "      <td>58</td>\n",
       "      <td>-1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://patentimages.storage.googleapis.com/f1...</td>\n",
       "      <td>Keyless flat panel portable computer--computer...</td>\n",
       "      <td>yes; x; written; writing; word</td>\n",
       "      <td>United States Patent   19   Dao et al.   11   ...</td>\n",
       "      <td>-1000.000000</td>\n",
       "      <td>51</td>\n",
       "      <td>-1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>http://www.ivanpoupyrev.com/e-library/2004/CHI...</td>\n",
       "      <td>Gummi: a bendable computer</td>\n",
       "      <td>zooming; zoom; would; world; work</td>\n",
       "      <td>Gummi  A Bendable Computer  Carsten Schwesig  ...</td>\n",
       "      <td>5.122833</td>\n",
       "      <td>57</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://paulbourke.net/fractals/peterdejong/pet...</td>\n",
       "      <td>Computer recreations</td>\n",
       "      <td>zones; xsin; x; wrote; write</td>\n",
       "      <td>COMPUTER RECREATIONS Author s   A. K. Dewdney ...</td>\n",
       "      <td>-1000.000000</td>\n",
       "      <td>60</td>\n",
       "      <td>-1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>https://nissenbaum.tech.cornell.edu/papers/How...</td>\n",
       "      <td>How computer systems embody values</td>\n",
       "      <td>yet; year; write; would; worry</td>\n",
       "      <td>T H E   P R O F E S S I O N  How Computer Syst...</td>\n",
       "      <td>-1000.000000</td>\n",
       "      <td>59</td>\n",
       "      <td>-1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Link  \\\n",
       "0           0  https://www.sonycsl.co.jp/person/rekimoto/pape...   \n",
       "1           1  https://apps.dtic.mil/dtic/tr/fulltext/u2/7078...   \n",
       "2           2  https://patentimages.storage.googleapis.com/f1...   \n",
       "3           3  http://www.ivanpoupyrev.com/e-library/2004/CHI...   \n",
       "4           4  http://paulbourke.net/fractals/peterdejong/pet...   \n",
       "5           5  https://nissenbaum.tech.cornell.edu/papers/How...   \n",
       "\n",
       "                                        Source title  \\\n",
       "0  The world through the computer: Computer augme...   \n",
       "1  THE ALOHA SYSTEM: another alternative for comp...   \n",
       "2  Keyless flat panel portable computer--computer...   \n",
       "3                         Gummi: a bendable computer   \n",
       "4                               Computer recreations   \n",
       "5                 How computer systems embody values   \n",
       "\n",
       "                      Index Keywords  \\\n",
       "0  x; world; workstation; work; wish   \n",
       "1     yj; xpl; written; would; world   \n",
       "2     yes; x; written; writing; word   \n",
       "3  zooming; zoom; would; world; work   \n",
       "4       zones; xsin; x; wrote; write   \n",
       "5     yet; year; write; would; worry   \n",
       "\n",
       "                                            Abstract         CAOT  SCA   SQM  \n",
       "0  The World through the Computer   Computer Augm...    19.476586   57   153  \n",
       "1  AFOqSR  7 0  16 8 6TA  THE  ALOHA  SYSTEM    A... -1000.000000   58 -1000  \n",
       "2  United States Patent   19   Dao et al.   11   ... -1000.000000   51 -1000  \n",
       "3  Gummi  A Bendable Computer  Carsten Schwesig  ...     5.122833   57    56  \n",
       "4  COMPUTER RECREATIONS Author s   A. K. Dewdney ... -1000.000000   60 -1000  \n",
       "5  T H E   P R O F E S S I O N  How Computer Syst... -1000.000000   59 -1000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "############################################### SCA ################################\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import treebank\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('cmudict')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#abstract=\"Among the many issues related to     data stream applications, those involved in predictive tasks such as classification and regression, play a significant role in Machine Learning (ML). The so-called ensemble-based approaches have characteristics that can be appealing to data stream applications, such as easy updating and high flexibility. In spite of that, some of the current approaches consider unsuitable ways of updating the ensemble along with the continuous stream processing, such as growing it indefinitely or deleting all its base learners when trying to overcome a concept drift. Such inadequate actions interfere with two inherent characteristics of data streams namely, its possible infinite length and its need for prompt responses. In this paper, a new ensemble-based algorithm, suitable for classification tasks, is proposed. It relies on applying boosting to new batches of data aiming at maintaining the ensemble by adding a certain number of base learners, which is established as a function of the current ensemble accuracy rate. The updating mechanism enhances the model flexibility, allowing the ensemble to gather knowledge fast to quickly overcome high error rates, due to concept drift, while maintaining satisfactory results by slowing down the updating rate in stable concepts. Results comparing the proposed ensemble-based algorithm against eight other ensembles found in the literature show that the proposed algorithm is very competitive when dealing with data stream classification. Â© 2018 Elsevier B.V.\"\n",
    "frequency_list = FreqDist(i.lower() for i in brown.words())\n",
    "\n",
    "def abstract_complexity(abstract):\n",
    "    #abstract=abstract.decode('utf-8')\n",
    "\n",
    "    sentences=sent_tokenize(abstract)\n",
    "    Ns=len(sentences)\n",
    "\n",
    "    d = cmudict.dict()\n",
    "    punctions=[u'.',u',',u'?',u'!',u'(',u')',u'\"',u';',u':',u'@',u'#',u'$',u'%',u'^',u'&',u'*',u'{',u'}',u'[',u']']\n",
    "    Nw=0\n",
    "    Nc=0\n",
    "    Nsy=0\n",
    "    Nhard=0\n",
    "    Nsimple=0\n",
    "    Navg=0\n",
    "\n",
    "    for s in sentences:\n",
    "        words=word_tokenize(s)\n",
    "        for w in words:\n",
    "            Nc=Nc+len(w)\n",
    "            val=frequency_list[w]\n",
    "            if w not in punctions:\n",
    "                if val>40000:\n",
    "                    Nsimple=Nsimple+1\n",
    "                else:\n",
    "                    if val<5000:\n",
    "                        Nhard=Nhard+1\n",
    "                    else:\n",
    "                        Navg=Navg+1\n",
    "            try:\n",
    "                sy=[len(list(y for y in x if y[-1].isdigit())) for x in d[w.lower()]]\n",
    "            except:\n",
    "                sy=[0]\n",
    "            Nsy=Nsy+sy[0]\n",
    "        Nw=Nw+len(words)\n",
    "        for p in punctions:\n",
    "            Nw=Nw-words.count(p)\n",
    "\n",
    "    sentences = nltk.sent_tokenize(abstract)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    Nch=0\n",
    "\n",
    "    def chunk(sentence):\n",
    "        chunkToExtract = \"\"\"\n",
    "        NP: {<NNP>*}\n",
    "            {<DT>?<JJ>?<NNS>}\n",
    "            {<NN><NN>}\"\"\"\n",
    "        parser = nltk.RegexpParser(chunkToExtract)\n",
    "        result = parser.parse(sentence)\n",
    "        N=0\n",
    "        for subtree in result.subtrees():\n",
    "            N=N+1\n",
    "        return N\n",
    "\n",
    "\n",
    "    for sentence in sentences:\n",
    "        Nch=Nch+chunk(sentence)\n",
    "\n",
    "    Nw=float(Nw)\n",
    "    Ns=float(Ns)\n",
    "    Nc=float(Nc)\n",
    "    Nsy=float(Nsy)\n",
    "    Nch=float(Nch)\n",
    "    Navg=float(Navg)\n",
    "    Nhard=float(Nhard)\n",
    "    Nsimple=float(Nsimple)\n",
    "\n",
    "    AvgWordsperSentence=Nw/Ns\n",
    "    AvgSyllablesperWord=Nsy/Nw\n",
    "\n",
    "    GulpeaseIndex= 89- 10*(Nc/Nw)+ 300*(Ns/Nw)\n",
    "\n",
    "    if Nch==1:\n",
    "        ChunkIndex=100\n",
    "    else:\n",
    "        ChunkIndex=100/((Nch/Ns)-1)\n",
    "\n",
    "    UnderstandabilityIndex=100*(Navg+0.75*Nsimple+0.5*Nhard)/Nw\n",
    "    return UnderstandabilityIndex\n",
    "    '''\n",
    "    print (\"Number of Sentences - \",Ns)\n",
    "    print (\"Number of Words - \",Nw)\n",
    "    print (\"Number of characters - \",Nc)\n",
    "    print (\"Avergae Number of Words per Sentence - \",AvgWordsperSentence)\n",
    "    print (\"Average Number of Syllables per Word - \",AvgSyllablesperWord)\n",
    "    print (\"Gulpease Index - \",GulpeaseIndex)\n",
    "    print (\"Chunk Index - \",ChunkIndex)\n",
    "    print (\"UnderstandabilityIndex - \",UnderstandabilityIndex)\n",
    "    '''\n",
    "#with open(\"data.csv\") as File:\n",
    "#    reader = csv.DictReader(File)\n",
    "#    results = [ row for row in reader ]\n",
    "#df = pd.read_csv(\"data.csv\")\n",
    "df[\"SCA\"] = 0\n",
    "    \n",
    "for i in range(len(df)):\n",
    "    df[\"SCA\"][i] = abstract_complexity(df[\"Abstract\"][i])\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
